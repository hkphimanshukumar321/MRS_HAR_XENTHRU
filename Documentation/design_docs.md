# Design and Implementation Details

## System Architecture
The project is organized into modular components corresponding to the stages of data processing and deployment:
- **Data Ingestion (fetch_data.py)**: Handles external dependencies (Kaggle). This separation ensures the core pipeline isn’t tied to Kaggle once data is downloaded.
- **Data Preprocessing (data_preprocessing.py)**: Focuses on domain-specific processing of radar signals. Keeping this in its own module allows data transformations to be easily modified or replaced (for example, swapping out background subtraction for a high-pass filter or adding a denoising autoencoder).
- **Feature Computation (compute_position.py)**: Though not strictly needed for classification, this module provides positional data that can enrich the system’s output. It runs independently of the main model pipeline, which means you could run it in parallel or even integrate it as an additional input to the model in the future (e.g., feeding the LSTM with position coordinates along with CNN features).
- **Model Definition (model.py)**: Abstracts the model architecture. We chose a CNN-LSTM for reasons discussed in the overview. By implementing both TensorFlow and PyTorch versions, the design allows experimentation in either environment. One could even train in PyTorch for flexibility and then convert the model to TensorFlow (using ONNX, etc.) for deployment on Coral, if needed.
- **Training (train.py)**: The training script is designed to handle the intricacies of cross-validation and transfer learning while keeping the main loop understandable. We deliberately avoided highly framework-specific training loops in favor of clarity. For example, we didn't use PyTorch Lightning or TensorFlow’s advanced callbacks here, to keep the learning process transparent. However, those tools could be integrated for more advanced use cases (like automated checkpointing, learning rate schedulers, etc.).
- **Inference (infer.py)**: Provides a clean interface to use the model. This could be wrapped into an API or a library function. The design choice to make it standalone ensures that loading and using the model does not inadvertently depend on training code or other heavy dependencies.
- **Real-Time Inference (real_time_inference.py)**: This is perhaps the most system-intensive part. The design needed to account for:
  - Continuous loop with minimal latency: using while True loops and ensuring as little overhead as possible in each iteration.
  - Integration with hardware accelerators: abstracted by checking `args.accelerator` and loading appropriate runtimes. This design allows easy extension; e.g., one could add `--accelerator tpu` for a hypothetical future TPU support, or `--accelerator gpu` if using Nvidia Jetson (with TensorRT, etc.).
  - Data buffering: we use a simple list as a circular buffer for frames. This is effective for moderate frame rates. If frame rate were very high, a deque or ring buffer might be more suitable. Also, thread safety isn't a big issue here since it’s single-threaded reading from one source. If multi-threading (producer/consumer) was needed, thread locks or a Queue would come into play.
  - Graceful handling of missing data: e.g., if the sensor temporarily doesn't have data, or if distances are not available for position, the code accounts for it by continuing or marking position as None.
- **Dashboard (dashboard.py)**: We separated the dashboard from the inference loop. An alternative design could have integrated the web server and inference in one process (for example, using Flask routes to trigger inference). We chose separation to decouple concerns: inference runs continuously regardless of whether a client is watching the dashboard, and the dashboard only reads the latest state. This also avoids multi-threading issues where Flask (which can handle multiple requests) might conflict with the sensor reading loop. The design with a shared file is a simple inter-process communication method. In a more scalable design, one might use a shared database or a pub-sub message broker, but that would add complexity not needed for a single-device local dashboard.
  
The directory structure (as outlined in the README) reinforces this modular architecture.

## Model and Algorithm Design
**CNN-LSTM Rationale:** A purely CNN approach (treating the entire sequence as a large 2D image, or doing 3D convolutions across time) was considered, but given the relatively small dataset and the sequential nature of activities, a CNN-LSTM was chosen for interpretability and ease of training. It also allows using pre-trained CNN components independently of temporal components (transfer learning benefit).

**Transfer Learning:** We included hooks for transfer learning anticipating scenarios like:
- Using a pre-trained CNN (e.g., on ImageNet or a radar imaging dataset) to initialize the convolution layers. The design in code uses `load_weights` with `by_name=True` in Keras, and `strict=False` in PyTorch to facilitate partial weight loading.
- Using the model trained on all but one subject as a starting point when training on the new subject. The LOSO loop could be extended: instead of training from scratch for each subject, one could start with a model pre-trained on a previous fold. We didn't implement a complex strategy here, but the provided option `--pretrained_model` allows experimentation. For instance, you could first train on a large external dataset, save that model, and then run `train.py` with `--pretrained_model` to fine-tune on this dataset.

**Trilateration:** The position calculation uses a straightforward linear algebra solution. This could be replaced by more sophisticated algorithms if needed (for example, if the environment has multipath issues or if the distances are noisy, one might use an Extended Kalman Filter to fuse distances over time for a smoother trajectory). For this HAR project, position is a secondary output, so the simple approach suffices and is robust given accurate distance inputs.

## Performance Considerations
During development, a focus was on ensuring the pipeline could run in near real-time:
- **Batching vs Streaming:** The training uses vectorized operations (especially in Keras) which is efficient for offline training. In contrast, the real-time inference processes one window at a time – this is a conscious design choice to minimize latency. We didn't batch multiple windows for throughput since the system is interactive (latency matters more than throughput here).
- **Memory Usage:** Storing the entire dataset in memory (as we did with `X, y` arrays) is fine for moderate sizes. If the dataset were much larger, we would redesign to use generators or dataset APIs that load data on the fly. For example, using `tf.data.Dataset` or PyTorch `Dataset` class to avoid memory blow-up. In our case, the dataset from Kaggle is assumed reasonably sized (likely a few hundred MBs at most).
- **Quantization:** The deployment model is quantized (for Coral) to int8, which slightly reduces accuracy but drastically improves speed. The design allows either int8 (for Coral) or FP16 (for NCS2) or even FP32 on CPU. One could include a step in training to quantize-aware training or at least evaluate the quantized model's accuracy to ensure it's acceptable.
- **Threading and Async:** The current design avoids multi-threading except where naturally provided (Flask can handle requests, OpenVINO runs internally asynchronous by default etc.). This is to keep things simple. If we needed to optimize further, possible improvements include:
  - Running the sensor reading in a separate thread and queuing frames to the inference thread (to parallelize I/O and compute).
  - Using asynchronous inference calls (OpenVINO’s exec_net.start_async for example) to queue the next inference while processing the current result.
  - Offloading the Flask app to a separate core or machine if the web interface impacts performance.
  
For our scope, these were not necessary, but the design leaves room for such enhancements.

## Future Directions
This design is modular and can be extended. Some ideas:
- **Additional Sensors:** The system could incorporate data from other sources, like adding a microphone for acoustic event detection or wearable sensors. The architecture could be expanded by adding new preprocessing modules and possibly combining data in the model (e.g., a multi-modal network). The current structure would allow adding another parallel pipeline for a new modality and then fusing in a new model.
- **Different Models:** One could try a Transformer-based model for sequence modeling instead of LSTM, or a Temporal Convolutional Network (TCN). The `model.py` can be adapted to define a different architecture (perhaps via a configuration file to choose model type).
- **Edge Device Optimization:** We targeted Coral and NCS2, but one might also consider NVIDIA Jetson (with TensorRT). The code structure would allow adding a branch in `real_time_inference.py` for TensorRT if needed.
- **AutoML and Hyperparameter Tuning:** Because we logged training progress, it’s feasible to run multiple experiments adjusting hyperparameters. One could integrate something like Optuna or Keras Tuner in the training process. The logs and results directories would be key in analyzing those runs.
- **Error Analysis:** Using the logs and possibly saving model predictions for each test sample can help analyze where the model is making mistakes. We might extend the training script to save the confusion matrix for each fold or save misclassified examples. This would go into `experiments/results` for examination and guide future improvements (like if many confusion between two activities, perhaps those activities need more distinctive features or data).
  
In summary, the design choices made ensure that each aspect of the HAR system can be understood, tested, and modified in isolation. This makes the codebase not only a solution for the current problem but also a baseline for future research or product development in UWB-based activity recognition.

""" Comments on Documentation: The project_overview.md is intended as a user-friendly README-style document. It gives context, usage instructions, and a summary of what’s achieved. The design_docs.md is more internally focused, discussing why certain decisions were made and how one could adapt the system. Keeping these separate means someone interested in just using the project can read the overview, while someone who wants to modify or extend it can delve into the design docs. Both documents should be maintained as the project evolves. For example, if you change the model or add new features, update the docs accordingly. Good documentation ensures that new contributors or users can quickly get up to speed with the project without having to read all the code immediately.
Final Note: All components of this codebase are interrelated but decoupled enough to allow independent testing. For example, you can test compute_position.py on its own with synthetic data, or run data_preprocessing.py and inspect the output before moving on to training. Each script has comments at the top explaining usage. This modular approach and thorough documentation/commenting will make future enhancements or debugging much easier. Feel free to customize paths, hyperparameters, and functions as needed – the code is meant to be a starting point that is adaptable to your specific HAR project needs. Good luck with your human activity recognition project using UWB radar! Enjoy experimenting and iterating on this codebase.
Copy
"""
